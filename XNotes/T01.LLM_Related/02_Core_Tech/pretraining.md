# 🔄 预训练方法与策略

## 🌟 预训练概述

- 📝 **定义**：在大规模无标注或弱标注数据上进行自监督学习的过程，为模型提供语言理解和生成的基础能力
- 🎯 **目标**：学习通用的语言表示，捕获语法结构、语义关系和世界知识
- 🧮 **规模特点**：通常涉及TB级别的文本数据和数十亿到数千亿参数的模型

## 📊 主流预训练任务类型

### 1️⃣ 自回归语言建模 (Autoregressive LM)

- 🧩 **任务描述**：预测序列中下一个token
- 📐 **数学表示**：最大化 $P(x_t|x_{<t})$
- 🔍 **代表模型**：GPT系列、LLaMA、Claude等
- ✅ **优势**：
  - 任务简单直接，不需要特殊数据准备
  - 自然适合生成任务
  - 可处理无限长文本
- ❌ **局限**：
  - 单向注意力限制了对上下文的全面理解
  - 难以有效捕获双向依赖关系

### 2️⃣ 掩码语言建模 (Masked LM)

- 🧩 **任务描述**：预测被随机掩盖的token
- 📐 **数学表示**：最大化 $P(x_{mask}|x_{unmask})$
- 🔍 **代表模型**：BERT及其变体
- ✅ **优势**：
  - 能够学习双向上下文
  - 擅长理解和表示任务
- ❌ **局限**：
  - 训练和推理存在差异
  - 不自然地适用于生成任务

### 3️⃣ 前缀语言建模 (Prefix LM)

- 🧩 **任务描述**：结合了双向注意力和自回归预测
- 📐 **工作方式**：在前缀部分使用双向注意力，在预测部分使用自回归
- 🔍 **代表模型**：UniLM、GLM
- ✅ **优势**：
  - 结合了MLM和自回归LM的优点
  - 适用于多种下游任务

### 4️⃣ 去噪自编码 (Denoising Autoencoding)

- 🧩 **任务描述**：恢复被破坏的文本
- 📐 **常见噪声类型**：
  - 随机掩码：如BERT的MLM
  - 文本删除和打乱：如BART、T5
  - 文本替换：用随机词替换原词
- 🔍 **代表模型**：BART、T5
- ✅ **优势**：
  - 学习更多语言规律和结构
  - 适合序列到序列任务

## 🧠 预训练数据处理

### 📚 数据来源与类型

- 🌐 **网络爬取数据**：Common Crawl、C4、WebText等
- 📖 **书籍与文献**：Books1, Books2、科学论文、报告
- 💬 **社交媒体**：Twitter、Reddit、微博等
- 🧮 **代码数据**：GitHub、Stack Overflow等
- 📃 **专业语料**：法律文本、医学文献、金融报告

### 🧹 数据清洗与筛选

- 🔍 **质量过滤**：
  - 基于启发式规则：如最小长度、重复内容比例
  - 基于统计指标：如困惑度、语句完整性
  - 基于模型评分：使用较小模型对数据质量打分
- 🧬 **去重处理**：
  - 精确重复：完全相同文本删除
  - 近似重复：MinHash、SimHash等算法
- 🛡️ **有害内容过滤**：
  - 敏感内容检测
  - NSFW内容过滤
  - 偏见与歧视内容识别

### 🧩 数据格式化与分词

- 🔤 **分词策略**：
  - 基于字节：如GPT-4的分词器
  - 基于词：WordPiece (BERT)、BPE (GPT)
  - 语言特定策略：中文单字/词混合
- 📏 **序列长度处理**：
  - 截断：限制最大长度
  - 动态批处理：相似长度文本放在一起
- 🏷️ **特殊标记添加**：
  - 文档分隔符
  - 语言标识符
  - 任务特定标记

## ⚙️ 预训练策略与优化

### 🔄 训练流程设计

- 🔢 **阶段设计**：
  - 单阶段：直接在大规模语料上训练
  - 多阶段：先在高质量小数据上训练，再在大规模数据上训练
  - 课程学习：从简单到复杂逐步增加难度
- 📊 **批量大小选择**：
  - 大批量：提高训练稳定性和并行效率
  - 梯度累积：克服硬件限制
- 🔍 **数据混合比例**：
  - 领域权重分配
  - 语言权重分配
  - 任务权重分配

### 🛠️ 训练优化技术

- 📈 **学习率策略**：
  - 预热 (Warmup)：避免早期不稳定
  - 衰减：线性、余弦、阶梯式
  - 周期性：余弦退火
- 🧮 **混合精度训练**：
  - FP16/BF16：提高训练速度和降低内存占用
  - 梯度缩放：防止精度损失
- 📊 **分布式训练**：
  - 数据并行：同样模型不同数据
  - 模型并行：模型分割到不同设备
  - 流水线并行：层间并行处理
  - ZeRO：优化器状态分片
- 🧠 **内存优化**：
  - 梯度检查点：牺牲计算换取内存
  - 激活值重计算：不存储中间激活
  - CPU卸载：临时将不使用的数据转移到CPU

### 🔍 预训练目标函数优化

- 🎯 **多任务混合**：
  - 自回归+掩码预测
  - 语言建模+其他预测任务
  - 不同权重分配
- 🧩 **辅助任务设计**：
  - 句子顺序预测
  - 文档级别连贯性建模
  - 实体关系预测

## 🌐 多语言预训练策略

- 🗣️ **语言均衡**：
  - 高资源语言降采样
  - 低资源语言增采样
  - 基于语言家族分组
- 🔄 **跨语言知识迁移**：
  - 语言连续预训练
  - 多语言词表设计
  - 语言标识符使用

## 📝 领域适应预训练

- 🏥 **垂直领域预训练**：
  - 医疗、法律、金融等专业领域继续预训练
  - 领域专用词表扩充
- 🎯 **任务特定预训练**：
  - 代码生成特化
  - 数学推理特化
  - 长文本处理特化

## 📊 预训练评估指标

- 📈 **困惑度 (Perplexity)**：
  - 测量模型预测下一个token的准确性
  - 越低表示模型预测越好
- 🎯 **零样本任务评估**：
  - 文本分类
  - 问答
  - 摘要生成
- 🔍 **中间检查点评估**：
  - 训练动态监控
  - 早期预测最终性能

## 💎 最新预训练趋势

- 🧠 **扩展上下文窗口**：
  - 位置编码改进
  - 注意力机制优化
  - 长序列特化训练
- 🤖 **指令预训练**：
  - 在预训练阶段加入指令遵循任务
  - 指令格式化数据增强
- 🌈 **多模态预训练**：
  - 文本+图像联合预训练
  - 跨模态对比学习
  - 统一表示空间构建

## 🔗 相关资源

- 📝 **论文**：
  - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (GPT-3)
  - [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- 🧮 **开源预训练代码**：
  - [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
  - [Hugging Face Transformers](https://github.com/huggingface/transformers)
- 📊 **预训练数据集**：
  - [The Pile](https://pile.eleuther.ai/)
  - [ROOTS](https://arxiv.org/abs/2303.03915) 