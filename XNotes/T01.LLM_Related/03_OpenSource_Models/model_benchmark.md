# 📊 开源大模型性能对比

## 📋 对比概述

本文档提供了主流开源大语言模型之间的性能对比，包括:
- 📏 基准测试成绩
- 💻 硬件需求与效率
- 🔍 功能支持与能力
- 💯 综合评分与适用场景

> 📝 注意：评测数据会随着模型版本更新而变化，本文基于截至2024年5月的公开数据。

## 🏆 基准测试成绩对比

### 📊 通用能力基准

以下表格展示了不同规模模型在主流基准测试上的表现（准确率%）:

| 模型 | 参数量 | MMLU | HellaSwag | ARC-C | GSM8K | HumanEval |
|------|-------|------|-----------|-------|-------|-----------|
| LLaMA 3 70B | 70B | 78.4 | 90.0 | 95.0 | 90.5 | 73.2 |
| LLaMA 3 8B | 8B | 68.4 | 87.3 | 85.7 | 81.2 | 46.3 |
| Claude 3 Haiku | ? | 75.0 | 87.1 | 93.4 | 84.1 | 69.5 |
| GPT-4 | ? | 85.5 | 95.3 | 96.3 | 92.0 | 67.0 |
| Mistral 8x7B | 56B | 70.6 | 87.3 | 89.8 | 69.0 | 40.2 |
| Qwen 72B | 72B | 77.5 | 87.1 | 94.0 | 78.9 | 55.5 |
| ChatGLM3 6B | 6B | 61.4 | 78.9 | 70.3 | 53.8 | 30.5 |
| LLaMA 2 70B | 70B | 68.9 | 85.5 | 85.2 | 56.8 | 29.9 |
| Mixtral 8x7B | 56B | 70.6 | 87.3 | 89.8 | 74.4 | 40.2 |

### 🀄 中文能力基准

以下表格展示了不同模型在中文评测基准上的表现（分数或正确率%）:

| 模型 | 参数量 | C-Eval | CMMLU | MMLU-ZH | BBH-CN | GSM8K-CN |
|------|-------|--------|-------|---------|--------|----------|
| Yi 34B | 34B | 81.8 | 83.0 | 75.5 | 66.4 | 81.2 |
| Qwen 72B | 72B | 83.3 | 85.2 | 78.5 | 71.5 | 78.9 |
| InternLM2 7B | 7B | 66.5 | 69.8 | 62.6 | 53.6 | 63.0 |
| ChatGLM3 6B | 6B | 69.0 | 71.1 | 63.9 | 56.3 | 72.6 |
| Baichuan2 13B | 13B | 58.1 | 61.3 | 55.9 | 41.0 | 52.8 |
| Llama 3 8B | 8B | 54.9 | 59.2 | 56.7 | 42.3 | 60.0 |
| Llama 2 70B | 70B | 41.0 | 45.5 | 43.3 | 38.5 | 42.1 |

### 💬 对话与指令跟随能力

以下表格展示了不同模型在对话评测中的表现（百分制）:

| 模型 | 参数量 | MT-Bench | AlpacaEval 2.0 | Chatbot Arena |
|------|-------|----------|----------------|---------------|
| LLaMA 3 70B | 70B | 8.20 | 37.4% | 1180 |
| Mistral Medium | ? | 8.61 | 34.8% | 1234 | 
| Claude 3 Opus | ? | 8.82 | 39.0% | 1275 |
| GPT-4o | ? | 9.30 | 48.6% | 1346 |
| Qwen 72B | 72B | 7.60 | 25.3% | 1120 |
| ChatGLM4 9B | 9B | 7.35 | 18.4% | - |
| Llama 2 70B | 70B | 6.86 | 13.6% | 1054 |

## 💻 计算资源需求对比

### 🖥️ 推理硬件需求

以下表格展示了不同模型的推理硬件需求:

| 模型 | 参数量 | 全精度GPU显存 | INT8量化 | INT4量化 | 推理速度(tokens/s) |
|------|-------|--------------|----------|----------|-------------------|
| LLaMA 3 70B | 70B | 140GB | 70GB | 35GB | 80 |
| LLaMA 3 8B | 8B | 16GB | 8GB | 4GB | 150 |
| ChatGLM3 6B | 6B | 13GB | 6.5GB | 3.3GB | 30 |
| Qwen 14B | 14B | 28GB | 14GB | 7GB | 40 |
| Mixtral 8x7B | 56B | 112GB | 56GB | 28GB | 30 |
| Baichuan2 13B | 13B | 26GB | 13GB | 6.5GB | 35 |

### ⚡ 训练资源需求

以下数据展示了训练不同规模模型所需的资源（估算值）:

| 参数规模 | GPU数量 (A100-80GB) | 训练时间 (tokens) | 估算成本 (USD) |
|---------|-------------------|------------------|---------------|
| 7B | 64 | 1.4万亿 | 50-100万 |
| 13B | 128 | 1.6万亿 | 100-200万 |
| 30B | 256 | 2万亿 | 300-500万 |
| 70B | 512 | 3.5万亿 | 800-1200万 |

## 🛠️ 功能支持对比

### 🧰 特殊能力支持

| 模型 | 工具调用 | 代码生成 | 长文本窗口 | 多模态 | 推理能力 | 中文支持 |
|------|---------|---------|-----------|--------|---------|---------|
| LLaMA 3 70B | ✅ | ✅++ | 8K | ❌ | ✅++ | ✅ |
| LLaMA 3 8B | ✅ | ✅ | 8K | ❌ | ✅ | ✅ |
| ChatGLM4 | ✅++ | ✅ | 128K | ✅ | ✅ | ✅++ |
| Yi 34B | ✅ | ✅+ | 4K | ❌ | ✅+ | ✅++ |
| Qwen 72B | ✅++ | ✅++ | 32K | ❌ | ✅+ | ✅++ |
| Mixtral 8x7B | ✅ | ✅+ | 32K | ❌ | ✅+ | ✅ |
| Baichuan2 13B | ✅ | ✅ | 4K | ❌ | ✅ | ✅++ |

> ✅: 支持 | ✅+: 较好 | ✅++: 优秀 | ❌: 不支持

### 🔐 许可证对比

| 模型系列 | 许可协议 | 商业使用 | 限制条件 |
|---------|---------|---------|---------|
| LLaMA 3 | Llama 3 许可 | ✅ | 安全限制，无责任保证 |
| ChatGLM3 | 商业许可 | ✅ | 需正当使用 |
| Qwen | Tongyi Qianwen许可 | ✅ | 安全限制，无责任保证 |
| Mistral/Mixtral | Apache 2.0 | ✅ | 最少限制 |
| Yi | Apache 2.0 | ✅ | 最少限制 |
| Baichuan2 | 商业许可 | ✅ | 需申请，有额外条款 |

## 💯 综合评估

### 🌟 不同规模模型推荐

**7B级别推荐**:
- 🥇 **最佳通用**: LLaMA 3 8B 
- 🥇 **最佳中文**: Qwen 7B / InternLM2 7B
- 🥇 **最佳代码**: CodeLlama 7B / DeepSeek Coder 7B

**13B级别推荐**:
- 🥇 **最佳通用**: Mistral 7B / Qwen 14B
- 🥇 **最佳中文**: Baichuan2 13B / ChatGLM3 6B
- 🥇 **最佳代码**: WizardCoder 13B / DeepSeek Coder 16B

**30B+级别推荐**:
- 🥇 **最佳通用**: Mixtral 8x7B / LLaMA 3 70B  
- 🥇 **最佳中文**: Yi 34B / Qwen 72B
- 🥇 **最佳代码**: DeepSeek Coder 33B

### 🎯 场景适用性推荐

**资源受限场景** (消费级GPU，如RTX 3090/4090):
- 🔍 **推荐模型**: ChatGLM3 6B, LLaMA 3 8B, Mistral 7B
- 💡 **优化建议**: 使用INT4量化，选择更小模型

**中文应用场景**:
- 🔍 **推荐模型**: Qwen系列, ChatGLM系列, Baichuan系列, Yi系列
- 💡 **优点**: 中文语料训练充分，文化理解更准确

**代码开发场景**:
- 🔍 **推荐模型**: DeepSeek Coder系列, CodeLlama系列, WizardCoder系列
- 💡 **优点**: 经过代码数据专门训练，代码生成和理解能力更强

**企业应用场景**:
- 🔍 **推荐模型**: LLaMA 3 70B, Mixtral 8x7B, Qwen 72B, Yi 34B
- 💡 **考量因素**: 资源需求、许可证、隐私安全

## 📊 评测方法说明

评测基准详细介绍:

- **MMLU**: 测量多学科知识的基准测试，包含57个科目，涵盖STEM、人文、社会科学和其他领域。
- **C-Eval**: 中文多学科知识测评基准，13948个多选题，涵盖四大学科领域52个学科。
- **GSM8K**: 小学数学应用题集合，测试模型的推理能力。
- **HumanEval**: 代码生成能力评测，需要根据函数描述生成正确的Python代码。
- **MT-Bench**: 多轮对话能力测评，模拟真实用户问题场景。
- **AlpacaEval**: 指令跟随能力评估，通过与强基准模型比较来评分。

## 📈 未来发展趋势

- 🚀 **模型规模持续增长**：参数量超过100B的开源模型将更加普及
- 🌐 **中文专精模型增多**：更多针对中文优化的高质量开源模型
- 💻 **推理效率提升**：更高效的量化和部署技术，降低硬件需求
- 🧩 **模块化架构**：MoE架构和专家模型的普及
- 🔧 **推理加速技术**：更多专门为大语言模型优化的硬件和软件
- 🤖 **垂直领域专精**：更多针对特定领域优化的开源模型

## 🔗 相关资源

- 📚 **评测平台**:
  - [OpenCompass](https://opencompass.org.cn/)
  - [LMSYS Chatbot Arena](https://chat.lmsys.org/)
  - [HuggingFace Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- 📊 **最新排行榜**:
  - [Open LLM 排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
  - [Chatbot Arena 排行榜](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
- 🔬 **学术论文**:
  - [LLM综合评测方法学](https://arxiv.org/abs/2310.05470)
  - [中文大模型能力分析](https://arxiv.org/abs/2304.10147) 