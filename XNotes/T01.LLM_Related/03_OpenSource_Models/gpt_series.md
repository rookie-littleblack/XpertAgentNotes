# ğŸŒŸ GPTç³»åˆ—æ¨¡å‹

## ğŸ“œ GPTæ¨¡å‹æ¦‚è¿°

- ğŸ” **å…¨ç§°**ï¼š**G**enerative **P**re-trained **T**ransformerï¼ˆç”Ÿæˆå¼é¢„è®­ç»ƒTransformerï¼‰
- ğŸ¢ **å¼€å‘ç»„ç»‡**ï¼šOpenAIï¼ˆéƒ¨åˆ†æ¨¡å‹æƒé‡å¼€æºï¼‰
- ğŸ§  **æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡å¤§è§„æ¨¡è‡ªå›å½’è¯­è¨€å»ºæ¨¡è¿›è¡Œé¢„è®­ç»ƒï¼Œå†é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒ
- ğŸ”„ **æ¶æ„ç‰¹ç‚¹**ï¼šä»…è§£ç å™¨Transformeræ¶æ„ï¼Œé‡‡ç”¨è‡ªå›å½’ç”Ÿæˆæ–¹å¼

## ğŸš€ æ¼”è¿›å†ç¨‹

### ğŸŒ± GPT-1 (2018)

- ğŸ“Š **è§„æ¨¡**ï¼š1.17äº¿å‚æ•°
- ğŸ“ **è®ºæ–‡**ï¼š[Improving Language Understanding by Generative Pre-training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- ğŸ’¡ **åˆ›æ–°ç‚¹**ï¼š
  - é¦–æ¬¡å°†é¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼åº”ç”¨äºTransformer
  - è¯æ˜äº†æ— ç›‘ç£é¢„è®­ç»ƒå¯¹ä¸‹æ¸¸NLPä»»åŠ¡çš„æœ‰æ•ˆæ€§
- ğŸŒ **æ•°æ®é›†**ï¼šBookCorpusæ•°æ®é›†ï¼ˆ7000æœ¬æœªå‡ºç‰ˆä¹¦ç±ï¼‰
- ğŸ“Š **è¡¨ç°**ï¼šåœ¨å¤šé¡¹NLPåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†SOTAç»“æœ

### ğŸŒ¿ GPT-2 (2019)

- ğŸ“Š **è§„æ¨¡**ï¼šä»1.24äº¿åˆ°15äº¿å‚æ•°ï¼ˆ4ç§è§„æ ¼ï¼‰
- ğŸ“ **è®ºæ–‡**ï¼š[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- ğŸ’¡ **åˆ›æ–°ç‚¹**ï¼š
  - é¦–æ¬¡å±•ç¤ºäº†é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›
  - è¯æ˜äº†æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œæ•°æ®é‡å¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½
  - Layer normalizationç§»è‡³æ¯ä¸ªå­å—çš„è¾“å…¥
- ğŸŒ **æ•°æ®é›†**ï¼šWebTextï¼ˆè¶…è¿‡800ä¸‡ç½‘é¡µï¼Œ40GBæ–‡æœ¬ï¼‰
- ğŸ”“ **å¼€æºæƒ…å†µ**ï¼šå®Œå…¨å¼€æºï¼Œä½†åˆ†é˜¶æ®µå‘å¸ƒï¼ˆåˆå§‹ä»…å‘å¸ƒæœ€å°ç‰ˆæœ¬ï¼‰

### ğŸŒ² GPT-3 (2020)

- ğŸ“Š **è§„æ¨¡**ï¼š1750äº¿å‚æ•°
- ğŸ“ **è®ºæ–‡**ï¼š[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- ğŸ’¡ **åˆ›æ–°ç‚¹**ï¼š
  - å±•ç¤ºäº†å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›
  - éªŒè¯äº†æ¶Œç°èƒ½åŠ›éšè§„æ¨¡å¢é•¿è€Œå‡ºç°
  - æ— éœ€å¾®è°ƒå³å¯é€‚åº”å¤šç§ä»»åŠ¡
- ğŸŒ **æ•°æ®é›†**ï¼šCommon Crawlã€WebText2ã€Books1&2ã€Wikipediaç­‰æ··åˆæ•°æ®é›†
- ğŸ”’ **å¼€æºæƒ…å†µ**ï¼šæœªå¼€æºï¼Œä»…é€šè¿‡APIè®¿é—®

### ğŸƒ GPT-Neo/GPT-J/GPT-NeoX (2021-2022)

- ğŸ“Š **è§„æ¨¡**ï¼š
  - GPT-Neoï¼š12.5äº¿å’Œ27äº¿å‚æ•°
  - GPT-Jï¼š60äº¿å‚æ•°
  - GPT-NeoXï¼š200äº¿å‚æ•°
- ğŸ¢ **å¼€å‘ç»„ç»‡**ï¼šEleutherAI
- ğŸ’¡ **ç‰¹ç‚¹**ï¼šå¯¹æ ‡GPT-3çš„å¼€æºå®ç°ï¼Œæ¶æ„ç•¥æœ‰è°ƒæ•´
- ğŸŒ **æ•°æ®é›†**ï¼šThe Pileï¼ˆ800GBé«˜è´¨é‡æ–‡æœ¬æ•°æ®é›†ï¼‰
- ğŸ”“ **å¼€æºæƒ…å†µ**ï¼šå®Œå…¨å¼€æºï¼Œæ¨¡å‹æƒé‡å¯å…è´¹ä¸‹è½½

### ğŸŒ¾ GPT-4 (2023)

- ğŸ“Š **è§„æ¨¡**ï¼šæœªå…¬å¼€ï¼Œä¼°è®¡ä¸‡äº¿çº§å‚æ•°
- ğŸ“ **è®ºæ–‡**ï¼š[GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)
- ğŸ’¡ **åˆ›æ–°ç‚¹**ï¼š
  - å¤šæ¨¡æ€èƒ½åŠ›ï¼ˆæ–‡æœ¬+å›¾åƒè¾“å…¥ï¼‰
  - æ›´å¼ºçš„æ¨ç†èƒ½åŠ›å’Œäº‹å®å‡†ç¡®æ€§
  - æ›´å¥½çš„å®‰å…¨å¯¹é½å’Œå‡å°‘æœ‰å®³è¾“å‡º
- ğŸ”’ **å¼€æºæƒ…å†µ**ï¼šæœªå¼€æºï¼Œä»…é€šè¿‡APIè®¿é—®

## ğŸ§© å¼€æºç‰ˆæœ¬è¯¦è§£

### ğŸ” GPT-2

- ğŸ› ï¸ **å¯ç”¨ç‰ˆæœ¬**ï¼š
  - å°å‹ (124M)
  - ä¸­å‹ (355M)
  - å¤§å‹ (774M)
  - è¶…å¤§å‹ (1.5B)
- ğŸ“Š **ç»“æ„å‚æ•°**ï¼š
  - n_layer = 12-48
  - n_head = 12-25
  - d_model = 768-1600
- ğŸ’¾ **ä¸‹è½½åœ°å€**ï¼š[HuggingFace GPT-2æ¨¡å‹](https://huggingface.co/gpt2)
- ğŸ“ **åº”ç”¨åœºæ™¯**ï¼š
  - æ–‡æœ¬ç”Ÿæˆ
  - ç®€å•é—®ç­”
  - æ•…äº‹åˆ›ä½œ
  - è¯­è¨€å»ºæ¨¡ç ”ç©¶

### ğŸŒ GPT-Neo/J/NeoX

- ğŸ› ï¸ **ä¸»è¦ç‰ˆæœ¬**ï¼š
  - GPT-Neo (1.3B/2.7B)
  - GPT-J (6B)
  - GPT-NeoX (20B)
- ğŸ“Š **æ¶æ„ç‰¹ç‚¹**ï¼š
  - å¹¶è¡Œæ³¨æ„åŠ›å±‚
  - æ—‹è½¬ä½ç½®ç¼–ç 
  - å…¨å±€å±‚å½’ä¸€åŒ–
- ğŸ’¾ **ä¸‹è½½åœ°å€**ï¼š
  - [GPT-Neo](https://huggingface.co/EleutherAI/gpt-neo-1.3B)
  - [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6b)
  - [GPT-NeoX](https://huggingface.co/EleutherAI/gpt-neox-20b)
- ğŸ“„ **è®¸å¯è¯**ï¼šApache 2.0
- ğŸ“ **åº”ç”¨åœºæ™¯**ï¼š
  - ä»£ç ç”Ÿæˆ
  - æ–‡æ¡£æ‘˜è¦
  - å¯¹è¯ç³»ç»Ÿ
  - å†…å®¹åˆ›ä½œ

## âš™ï¸ æ¨¡å‹æ¶æ„åˆ†æ

### ğŸ§® ä¸»è¦æ¶æ„ç»„ä»¶

- ğŸ”„ **è‡ªå›å½’è§£ç å™¨**ï¼šä»å·¦åˆ°å³é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
- ğŸ‘ï¸ **å› æœè‡ªæ³¨æ„åŠ›**ï¼šæ¯ä¸ªtokenåªå…³æ³¨å…¶å‰é¢çš„token
- ğŸ”„ **æ®‹å·®è¿æ¥**ï¼šæ¯ä¸ªå­å±‚åæ·»åŠ ï¼Œå¸®åŠ©æ¢¯åº¦æµåŠ¨
- ğŸ“Š **å±‚å½’ä¸€åŒ–**ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
- ğŸ§  **è¯åµŒå…¥å±‚**ï¼šå°†tokenè½¬åŒ–ä¸ºå‘é‡è¡¨ç¤º
- ğŸ“ **ä½ç½®ç¼–ç **ï¼šæä¾›åºåˆ—ä½ç½®ä¿¡æ¯

### ğŸ“ˆ è§„æ¨¡æ‰©å±•ç­–ç•¥

- ğŸ§® **æ·±åº¦æ‰©å±•**ï¼šå¢åŠ Transformerå±‚æ•°
- ğŸ“ **å®½åº¦æ‰©å±•**ï¼šå¢åŠ éšè—å±‚ç»´åº¦å’Œæ³¨æ„åŠ›å¤´æ•°
- ğŸ“Š **è¯è¡¨æ‰©å±•**ï¼šå¢åŠ åˆ†è¯å™¨è¯è¡¨å¤§å°
- ğŸ’¡ **ä¼˜åŒ–æ”¹è¿›**ï¼šæ›´å¥½çš„åˆå§‹åŒ–å’Œæ­£åˆ™åŒ–æŠ€æœ¯

## ğŸ’» å®é™…åº”ç”¨ç¤ºä¾‹

### ğŸ› ï¸ ä½¿ç”¨Hugging FaceåŠ è½½GPT-2

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# ç”Ÿæˆæ–‡æœ¬
input_text = "äººå·¥æ™ºèƒ½å°†åœ¨æœªæ¥"
input_ids = tokenizer.encode(input_text, return_tensors='pt')
output = model.generate(input_ids, 
                        max_length=100, 
                        num_return_sequences=1, 
                        no_repeat_ngram_size=2)
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)
```

### ğŸ¯ å¾®è°ƒGPT-2ç¤ºä¾‹

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# å‡†å¤‡æ•°æ®é›†
train_dataset = TextDataset(
    tokenizer=tokenizer,
    file_path="train.txt",
    block_size=128)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, 
    mlm=False)

# è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
)

# åˆå§‹åŒ–Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
)

# å¼€å§‹å¾®è°ƒ
trainer.train()

# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
model.save_pretrained("./gpt2-finetuned")
tokenizer.save_pretrained("./gpt2-finetuned")
```

## ğŸ” GPTç³»åˆ—ä¼˜ç¼ºç‚¹åˆ†æ

### âœ… ä¼˜åŠ¿

- ğŸš€ **ç”Ÿæˆè´¨é‡é«˜**ï¼šæ–‡æœ¬è¿è´¯æ€§å’Œå¤šæ ·æ€§ä¼˜ç§€
- ğŸ§  **é€šç”¨æ€§å¼º**ï¼šé€‚ç”¨äºå¤šç§NLPä»»åŠ¡
- ğŸ“š **èµ„æºä¸°å¯Œ**ï¼šå¤§é‡æ•™ç¨‹å’Œç¤¾åŒºæ”¯æŒ
- ğŸ”„ **æ˜“äºå¾®è°ƒ**ï¼šå„ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•é€‚ç”¨

### âŒ åŠ£åŠ¿

- ğŸ’¾ **èµ„æºéœ€æ±‚é«˜**ï¼šå¤§æ¨¡å‹ç‰ˆæœ¬éœ€è¦å¤§é‡è®¡ç®—èµ„æº
- ğŸ§¿ **å¹»è§‰é—®é¢˜**ï¼šå¯èƒ½ç”Ÿæˆè™šæ„æˆ–ä¸å‡†ç¡®ä¿¡æ¯
- â±ï¸ **æ¨ç†é€Ÿåº¦**ï¼šå¤§æ¨¡å‹æ¨ç†è¾ƒæ…¢ï¼Œä¸é€‚åˆå®æ—¶åº”ç”¨
- ğŸ”’ **æœ€æ–°æŠ€æœ¯é—­æº**ï¼šæœ€å…ˆè¿›çš„GPT-4ç­‰æ¨¡å‹æœªå¼€æº

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

| æ¨¡å‹ | MMLU | HumanEval | HellaSwag | TruthfulQA | æ¨ç†é€Ÿåº¦(tokens/s) |
|------|------|-----------|-----------|------------|-------------------|
| GPT-2 (1.5B) | 32.6% | 13.1% | 67.4% | 36.2% | ~30 |
| GPT-Neo (2.7B) | 33.2% | 15.4% | 70.2% | 38.5% | ~25 |
| GPT-J (6B) | 35.6% | 18.5% | 72.8% | 40.3% | ~18 |
| GPT-NeoX (20B) | 37.8% | 23.1% | 77.5% | 42.7% | ~8 |

## ğŸ”— ç›¸å…³èµ„æº

- ğŸ“ **å®˜æ–¹èµ„æº**ï¼š
  - [OpenAI GPT-2è®ºæ–‡](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  - [EleutherAI](https://www.eleuther.ai/)
- ğŸ’» **ä»£ç å®ç°**ï¼š
  - [Hugging Face Transformers](https://github.com/huggingface/transformers)
  - [GPT-NeoX GitHub](https://github.com/EleutherAI/gpt-neox)
- ğŸ“š **æ•™ç¨‹ä¸å®ä¾‹**ï¼š
  - [GPT-2å¾®è°ƒæŒ‡å—](https://huggingface.co/blog/how-to-generate)
  - [GPTæ¨¡å‹ä½¿ç”¨æœ€ä½³å®è·µ](https://huggingface.co/docs/transformers/model_doc/gpt2) 