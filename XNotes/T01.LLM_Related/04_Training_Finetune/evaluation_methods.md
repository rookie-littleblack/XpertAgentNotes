# ğŸ” å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ³•

## ğŸ“‹ è¯„ä¼°æ¦‚è¿°

### ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦è¯„ä¼°

è¯„ä¼°æ˜¯å¤§è¯­è¨€æ¨¡å‹(LLM)å¼€å‘ä¸åº”ç”¨çš„å…³é”®ç¯èŠ‚ï¼Œå®ƒå¸®åŠ©æˆ‘ä»¬ï¼š
- ğŸ“Š **é‡åŒ–æ¨¡å‹æ€§èƒ½**ï¼šå®¢è§‚è¡¡é‡æ¨¡å‹èƒ½åŠ›
- ğŸ” **è¯†åˆ«æ¨¡å‹å¼±ç‚¹**ï¼šæ‰¾å‡ºéœ€è¦æ”¹è¿›çš„æ–¹å‘
- ğŸ“ˆ **è·Ÿè¸ªç ”ç©¶è¿›å±•**ï¼šæ¯”è¾ƒä¸åŒæ¨¡å‹å’Œæ–¹æ³•
- ğŸ›¡ï¸ **é™ä½éƒ¨ç½²é£é™©**ï¼šè¯„ä¼°æ¨¡å‹å®‰å…¨æ€§å’Œå¯é æ€§
- ğŸ’¼ **æŒ‡å¯¼å®é™…åº”ç”¨**ï¼šé€‰æ‹©æœ€é€‚åˆç‰¹å®šåœºæ™¯çš„æ¨¡å‹

### ğŸŒŸ è¯„ä¼°çš„æŒ‘æˆ˜æ€§

è¯„ä¼°LLMé¢ä¸´è¯¸å¤šç‹¬ç‰¹æŒ‘æˆ˜ï¼š
- ğŸ”„ **å¼€æ”¾æ€§ç”Ÿæˆ**ï¼šè¾“å‡ºå¤šæ ·ï¼Œéš¾ä»¥å®šä¹‰å”¯ä¸€"æ­£ç¡®"ç­”æ¡ˆ
- ğŸ“ **èƒ½åŠ›å¤šç»´æ€§**ï¼šéœ€è¯„ä¼°å¤šç§èƒ½åŠ›ç»´åº¦
- ğŸ§  **çŸ¥è¯†æ—¶æ•ˆæ€§**ï¼šçŸ¥è¯†å¯èƒ½è¿‡æ—¶æˆ–ä¸å‡†ç¡®
- ğŸŒ **æ–‡åŒ–é€‚åº”æ€§**ï¼šä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹è¡¨ç°å·®å¼‚
- ğŸ“š **æ•°æ®æ±¡æŸ“é—®é¢˜**ï¼šæµ‹è¯•é›†å¯èƒ½å·²è¢«æ¨¡å‹è®­ç»ƒè¿‡

## ğŸ§ª è¯„ä¼°æ¡†æ¶ä¸ç»´åº¦

### 1. ğŸ“Š èƒ½åŠ›è¯„ä¼°ç»´åº¦

**åŸºç¡€è¯­è¨€èƒ½åŠ›**ï¼š
- ğŸ”¤ **è¯­æ³•ä¸æµç•…æ€§**ï¼šè¯­è¨€è¡¨è¾¾æ˜¯å¦è‡ªç„¶ã€è¯­æ³•æ­£ç¡®
- ğŸ“š **è¯æ±‡ä¸°å¯Œåº¦**ï¼šè¯æ±‡ä½¿ç”¨çš„å¤šæ ·æ€§å’Œå‡†ç¡®æ€§
- ğŸ§  **è¯­ä¹‰ç†è§£**ï¼šç†è§£å¥å­å’Œæ®µè½çš„å«ä¹‰

**é«˜çº§è®¤çŸ¥èƒ½åŠ›**ï¼š
- ğŸ’¡ **æ¨ç†èƒ½åŠ›**ï¼šé€»è¾‘æ¨ç†ã€å› æœå…³ç³»ç†è§£
- ğŸ§® **æ•°å­¦èƒ½åŠ›**ï¼šè®¡ç®—ã€æ–¹ç¨‹æ±‚è§£ã€æ•°å­¦æ¨å¯¼
- ğŸ” **å¸¸è¯†åˆ¤æ–­**ï¼šåŸºæœ¬å¸¸è¯†ä¸ä¸–ç•ŒçŸ¥è¯†
- ğŸ“ **åˆ›æ„ç”Ÿæˆ**ï¼šåˆ›æ–°æ€§å’ŒåŸåˆ›æ€§å†…å®¹ç”Ÿæˆ

**ä¸“ä¸šé¢†åŸŸèƒ½åŠ›**ï¼š
- ğŸ’» **ç¼–ç¨‹èƒ½åŠ›**ï¼šä»£ç ç”Ÿæˆä¸ç†è§£
- ğŸ¥ **åŒ»å­¦çŸ¥è¯†**ï¼šè¯Šæ–­ã€æ²»ç–—å»ºè®®ç­‰
- âš–ï¸ **æ³•å¾‹ç†è§£**ï¼šæ³•å¾‹æ¡æ–‡è§£é‡Šã€æ¡ˆä¾‹åˆ†æ
- ğŸ”¬ **ç§‘å­¦å‡†ç¡®æ€§**ï¼šç§‘å­¦æ¦‚å¿µè§£é‡Šçš„å‡†ç¡®åº¦

**äººç±»å¯¹é½èƒ½åŠ›**ï¼š
- ğŸ¤ **æŒ‡ä»¤éµå¾ª**ï¼šç†è§£å¹¶æ‰§è¡Œå„ç§æŒ‡ä»¤
- ğŸ›¡ï¸ **å®‰å…¨æ€§**ï¼šé¿å…æœ‰å®³ã€ä¸å½“å†…å®¹ç”Ÿæˆ
- ğŸ§© **æœ‰ç”¨æ€§**ï¼šæä¾›æœ‰å¸®åŠ©çš„å›åº”
- ğŸ­ **çœŸå®æ€§**ï¼šç”ŸæˆçœŸå®è€Œéè™šæ„çš„ä¿¡æ¯

### 2. ğŸ§© è¯„ä¼°æ–¹æ³•åˆ†ç±»

#### a. ğŸ† åŸºå‡†æµ‹è¯•è¯„ä¼°

**ç‰¹ç‚¹**ï¼š
- é¢„å®šä¹‰çš„æ ‡å‡†åŒ–æµ‹è¯•é›†
- å®¢è§‚è¯„åˆ†æ ‡å‡†
- ä¾¿äºæ¨¡å‹é—´æ¯”è¾ƒ
- é€šå¸¸å…³æ³¨ç‰¹å®šèƒ½åŠ›ç»´åº¦

**å®ç°æ–¹å¼**ï¼š
```python
# ä½¿ç”¨åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹
from lm_eval import evaluator

# é…ç½®è¯„ä¼°å‚æ•°
eval_config = {
    "model": "your_model_name",
    "tasks": ["mmlu", "gsm8k", "truthfulqa"],
    "batch_size": 16
}

# æ‰§è¡Œè¯„ä¼°
results = evaluator.simple_evaluate(**eval_config)
print(results)
```

#### b. ğŸ‘¥ äººå·¥è¯„ä¼°

**ç‰¹ç‚¹**ï¼š
- äººç±»è¯„ä»·è€…ç›´æ¥è¯„åˆ¤æ¨¡å‹å›ç­”
- å¯è¯„ä¼°ä¸»è§‚è´¨é‡ç»´åº¦(åˆ›æ„æ€§ã€æœ‰ç”¨æ€§ç­‰)
- æˆæœ¬é«˜ï¼Œä½†æ›´è´´è¿‘å®é™…ä½¿ç”¨ä½“éªŒ
- å¯èƒ½å—è¯„ä»·è€…åè§å½±å“

**è¯„ä»·æ–¹å¼**ï¼š
- ç»å¯¹è¯„åˆ†ï¼šå¯¹å•ä¸€å›ç­”æŒ‰è¯„åˆ†æ ‡å‡†æ‰“åˆ†
- ç›¸å¯¹æ’åºï¼šæ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„å›ç­”ä¼˜åŠ£
- A/Bæµ‹è¯•ï¼šç›²æµ‹ä¸çŸ¥é“æ¥æºçš„å›ç­”

#### c. ğŸ¤– æ¨¡å‹è¾…åŠ©è¯„ä¼°

**ç‰¹ç‚¹**ï¼š
- åˆ©ç”¨å¼ºå¤§æ¨¡å‹(å¦‚GPT-4)è¯„ä¼°å…¶ä»–æ¨¡å‹
- æˆæœ¬ä½äºäººå·¥ï¼Œé«˜äºè‡ªåŠ¨æŒ‡æ ‡
- å¯æ‰©å±•æ€§å¼ºï¼Œå¯è¯„ä¼°å¤§é‡æ ·æœ¬
- å¯èƒ½ç»§æ‰¿è¾…åŠ©æ¨¡å‹çš„åè§

**å®ç°ç¤ºä¾‹**ï¼š
```python
import openai

def model_evaluate_response(prompt, response, criteria):
    """ä½¿ç”¨GPT-4è¯„ä¼°æ¨¡å‹å›ç­”"""
    evaluation_prompt = f"""
    è¯·è¯„ä¼°ä»¥ä¸‹å›ç­”çš„è´¨é‡ã€‚
    
    é—®é¢˜: {prompt}
    å›ç­”: {response}
    
    è¯„ä¼°ç»´åº¦: {criteria}
    è¯·ä¸ºæ¯ä¸ªç»´åº¦æ‰“åˆ†(1-10)å¹¶æä¾›è§£é‡Šã€‚
    """
    
    result = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„AIè¯„ä¼°ä¸“å®¶ã€‚"},
            {"role": "user", "content": evaluation_prompt}
        ]
    )
    
    return result.choices[0].message.content
```

#### d. ğŸ“ˆ è‡ªåŠ¨åŒ–æŒ‡æ ‡è¯„ä¼°

**ç‰¹ç‚¹**ï¼š
- åŸºäºé¢„å®šä¹‰ç®—æ³•çš„è‡ªåŠ¨è¯„åˆ†
- è®¡ç®—é«˜æ•ˆï¼Œå¯å¤§è§„æ¨¡åº”ç”¨
- å®¢è§‚ä¸€è‡´ï¼Œä¸å—ä¸»è§‚å› ç´ å½±å“
- å¯¹æŸäº›èƒ½åŠ›ç»´åº¦è¯„ä¼°æœ‰é™

**å¸¸ç”¨æŒ‡æ ‡**ï¼š
- BLEU/ROUGEï¼šè¯„ä¼°ç”Ÿæˆæ–‡æœ¬ä¸å‚è€ƒæ–‡æœ¬çš„ç›¸ä¼¼åº¦
- å‡†ç¡®ç‡/F1åˆ†æ•°ï¼šè¯„ä¼°åˆ†ç±»å’Œé—®ç­”ä»»åŠ¡
- å›°æƒ‘åº¦(Perplexity)ï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹å¯¹æ–‡æœ¬çš„é¢„æµ‹èƒ½åŠ›

## ğŸ“Š ä¸»è¦è¯„ä¼°åŸºå‡†æµ‹è¯•

### 1. ğŸ§  é€šç”¨çŸ¥è¯†ä¸ç†è§£åŠ›

#### MMLU (Massive Multitask Language Understanding)

**æ¦‚è¿°**ï¼šæµ‹é‡å¤šå­¦ç§‘çŸ¥è¯†çš„ç»¼åˆæµ‹è¯•

**ç‰¹ç‚¹**ï¼š
- 57ä¸ªå­¦ç§‘ï¼ŒåŒ…æ‹¬STEMã€äººæ–‡ã€ç¤¾ä¼šç§‘å­¦ç­‰
- å¤šé€‰é¢˜æ ¼å¼ï¼Œæµ‹è¯•åŸºç¡€çŸ¥è¯†å’Œé«˜çº§ç†è§£
- å¹¿æ³›è¢«è®¤ä¸ºæ˜¯LLMçŸ¥è¯†å¹¿åº¦çš„é‡è¦æŒ‡æ ‡

**ç¤ºä¾‹é¢˜ç›®**ï¼š
> é—®é¢˜ï¼šæ ¹æ®å‹’æ²™ç‰¹åˆ—åŸç†ï¼Œä¸‹åˆ—å“ªç§æƒ…å†µä¼šä½¿å¹³è¡¡å‘ç”Ÿæˆæ›´å¤šäº§ç‰©æ–¹å‘ç§»åŠ¨ï¼Ÿ
> 1. å¢åŠ ååº”ç‰©æµ“åº¦
> 2. å¢åŠ äº§ç‰©æµ“åº¦
> 3. å¢åŠ æ¸©åº¦ï¼ˆå¯¹äºå¸çƒ­ååº”ï¼‰
> 4. å¢åŠ æ¸©åº¦ï¼ˆå¯¹äºæ”¾çƒ­ååº”ï¼‰

#### TruthfulQA

**æ¦‚è¿°**ï¼šæµ‹è¯•æ¨¡å‹æä¾›çœŸå®ã€å‡†ç¡®ä¿¡æ¯çš„èƒ½åŠ›

**ç‰¹ç‚¹**ï¼š
- ä¸“æ³¨äºå¸¸è§è¯¯è§£å’Œè™šå‡ä¿¡æ¯åœºæ™¯
- è¯„ä¼°æ¨¡å‹æ˜¯å¦ä¼šä¼ æ’­é”™è¯¯ä¿¡æ¯
- åŒæ—¶æµ‹è¯•çœŸå®æ€§å’Œä¿¡æ¯æ€§

**ç¤ºä¾‹é¢˜ç›®**ï¼š
> é—®é¢˜ï¼šæ‘©è¥¿å¸¦äº†å¤šå°‘åŠ¨ç‰©ä¸Šæ–¹èˆŸï¼Ÿ
> ï¼ˆæ³¨ï¼šå®é™…ä¸Šæ˜¯è¯ºäºšè€Œéæ‘©è¥¿å¸¦åŠ¨ç‰©ä¸Šæ–¹èˆŸï¼Œè¿™é‡Œæµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½è¯†åˆ«é—®é¢˜ä¸­çš„é”™è¯¯å‡è®¾ï¼‰

### 2. ğŸ’¡ æ¨ç†ä¸é—®é¢˜è§£å†³

#### GSM8K (Grade School Math 8K)

**æ¦‚è¿°**ï¼šå°å­¦æ•°å­¦åº”ç”¨é¢˜é›†åˆï¼Œæµ‹è¯•æ•°å­¦æ¨ç†èƒ½åŠ›

**ç‰¹ç‚¹**ï¼š
- 8.5Kæ¡å¤šæ­¥éª¤æ•°å­¦é—®é¢˜
- éœ€è¦é€æ­¥æ¨ç†å’ŒåŸºæœ¬è¿ç®—
- æµ‹è¯•æ¨¡å‹çš„æ€ç»´é“¾èƒ½åŠ›

**ç¤ºä¾‹é¢˜ç›®**ï¼š
> é—®é¢˜ï¼šå°æ˜æœ‰5ä¸ªè‹¹æœã€‚ä»–ç»™äº†å°çº¢2ä¸ªï¼Œç„¶ååˆä»å•†åº—ä¹°äº†3ä¸ªã€‚ä¹‹åä»–åƒäº†1ä¸ªã€‚ä»–ç°åœ¨æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ

#### BBH (BIG-Bench Hard)

**æ¦‚è¿°**ï¼šä»BIG-Benchä¸­é€‰å–çš„å›°éš¾ä»»åŠ¡é›†åˆ

**ç‰¹ç‚¹**ï¼š
- 23ä¸ªå¤æ‚æ¨ç†ä»»åŠ¡
- åŒ…æ‹¬é€»è¾‘æ¨ç†ã€å¸¸è¯†æ¨ç†ã€ç®—æ³•æ€ç»´ç­‰
- å¯¹æ¨¡å‹é«˜çº§æ€ç»´èƒ½åŠ›çš„ç»¼åˆæµ‹è¯•

**ä»»åŠ¡ç¤ºä¾‹**ï¼š
- å½¢å¼æ¨ç†ï¼šæ ¹æ®å‰ææ¨å¯¼å‡ºé€»è¾‘ç»“è®º
- æ—¥æœŸç†è§£ï¼šå¤æ‚æ—¥æœŸè®¡ç®—é—®é¢˜
- å¤šæ­¥éª¤æ¨ç†ï¼šéœ€è¦å¤šä¸ªæ­¥éª¤æ‰èƒ½è§£å†³çš„é—®é¢˜

### 3. ğŸ’» ä»£ç ä¸ç®—æ³•

#### HumanEval

**æ¦‚è¿°**ï¼šå‡½æ•°è¡¥å…¨ä»»åŠ¡ï¼Œæµ‹è¯•ä»£ç ç”Ÿæˆèƒ½åŠ›

**ç‰¹ç‚¹**ï¼š
- 164ä¸ªæ‰‹å·¥ç¼–å†™çš„ç¼–ç¨‹é—®é¢˜
- åŒ…å«å‡½æ•°æè¿°å’Œæµ‹è¯•ç”¨ä¾‹
- è¯„ä¼°é€šè¿‡ç‡(pass@k)ï¼šæ¨¡å‹ç”Ÿæˆkä¸ªæ ·æœ¬ä¸­è‡³å°‘æœ‰ä¸€ä¸ªé€šè¿‡æµ‹è¯•çš„æ¦‚ç‡

**ç¤ºä¾‹ä»»åŠ¡**ï¼š
```python
def sorted_list_sum(lst):
    """
    ç»™å®šä¸€ä¸ªæ•°å­—åˆ—è¡¨ï¼ŒæŒ‰å‡åºæ’åºåè¿”å›åˆ—è¡¨ä¸­æœ€å°å’Œæœ€å¤§å…ƒç´ çš„å’Œã€‚
    >>> sorted_list_sum([1, 5, 3, 2])
    3
    >>> sorted_list_sum([])
    0
    """
    # éœ€è¦æ¨¡å‹è¡¥å…¨çš„å‡½æ•°ä½“
```

#### MBPP (Mostly Basic Python Programming)

**æ¦‚è¿°**ï¼šåŸºç¡€Pythonç¼–ç¨‹ä»»åŠ¡é›†

**ç‰¹ç‚¹**ï¼š
- 974ä¸ªç®€å•ç¼–ç¨‹é—®é¢˜
- æ¯é¢˜åŒ…å«å‡½æ•°æè¿°ã€ç¤ºä¾‹å’Œæµ‹è¯•ç”¨ä¾‹
- æ¯”HumanEvalæ›´åŸºç¡€ï¼Œè¦†ç›–é¢æ›´å¹¿

**ç¤ºä¾‹ä»»åŠ¡**ï¼š
```python
# ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ£€æŸ¥ç»™å®šå­—ç¬¦ä¸²æ˜¯å¦ä¸ºå›æ–‡
# ç¤ºä¾‹: is_palindrome("radar") -> True, is_palindrome("hello") -> False
```

### 4. ğŸ—£ï¸ å¯¹è¯ä¸æŒ‡ä»¤è·Ÿéš

#### MT-Bench (Multi-turn Benchmark)

**æ¦‚è¿°**ï¼šå¤šè½®å¯¹è¯èƒ½åŠ›è¯„ä¼°åŸºå‡†

**ç‰¹ç‚¹**ï¼š
- 80ä¸ªé«˜è´¨é‡å¤šè½®å¯¹è¯é—®é¢˜
- æ¶µç›–å†™ä½œã€æ¨ç†ã€æå–ã€æ•°å­¦ç­‰èƒ½åŠ›
- ä½¿ç”¨GPT-4è¯„åˆ†(1-10åˆ†)

**ç¤ºä¾‹å¯¹è¯**ï¼š
```
User: å¸®æˆ‘å†™ä¸€å°é‚®ä»¶ç»™æˆ‘çš„å›¢é˜Ÿï¼Œå‘ŠçŸ¥ä¸‹å‘¨ä¼šè®®å–æ¶ˆã€‚
Assistant: [æ¨¡å‹å›ç­”ç¬¬ä¸€è½®]
User: ä½¿é‚®ä»¶è¯­æ°”æ›´æ­£å¼ï¼Œå¹¶æ·»åŠ ä¸€äº›å…³äºä¸ºä»€ä¹ˆå–æ¶ˆçš„è§£é‡Šã€‚
Assistant: [æ¨¡å‹å›ç­”ç¬¬äºŒè½®]
```

#### AlpacaEval

**æ¦‚è¿°**ï¼šåŸºäºåå¥½çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›è¯„ä¼°

**ç‰¹ç‚¹**ï¼š
- ä½¿ç”¨å¼ºæ¨¡å‹ä½œä¸ºè¯„åˆ¤æ ‡å‡†
- è®¡ç®—"è·èƒœç‡"ï¼šæ¨¡å‹å›ç­”ä¼˜äºå‚è€ƒæ¨¡å‹çš„æ¯”ä¾‹
- å¹¿æ³›ç”¨äºè¯„ä¼°æŒ‡ä»¤è°ƒæ•´æ•ˆæœ

**è¯„ä¼°æµç¨‹**ï¼š
1. æ”¶é›†å¤šæ ·åŒ–æŒ‡ä»¤é›†
2. è®©è¢«è¯„ä¼°æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹åˆ†åˆ«å›ç­”
3. ä½¿ç”¨è¯„åˆ¤æ¨¡å‹æ¯”è¾ƒä¸¤ä¸ªå›ç­”çš„è´¨é‡
4. è®¡ç®—è¢«è¯„ä¼°æ¨¡å‹"è·èƒœ"çš„æ¯”ä¾‹

### 5. ğŸŒ å¤šè¯­è¨€è¯„ä¼°

#### MGSM (Multilingual Grade School Math)

**æ¦‚è¿°**ï¼šGSM8Kçš„å¤šè¯­è¨€ç‰ˆæœ¬

**ç‰¹ç‚¹**ï¼š
- åŒ…å«ä¸­æ–‡ã€æ—¥è¯­ã€éŸ©è¯­ç­‰å¤šç§è¯­è¨€
- æµ‹è¯•è·¨è¯­è¨€çš„æ•°å­¦æ¨ç†èƒ½åŠ›
- è¯„ä¼°æ¨¡å‹çš„å¤šè¯­è¨€æ³›åŒ–èƒ½åŠ›

#### FLORES-200

**æ¦‚è¿°**ï¼šå¤šè¯­è¨€ç¿»è¯‘èƒ½åŠ›è¯„ä¼°

**ç‰¹ç‚¹**ï¼š
- è¦†ç›–200ç§è¯­è¨€çš„ç¿»è¯‘å¯¹
- ä¸“ä¸šäººå·¥ç¿»è¯‘çš„é«˜è´¨é‡å‚è€ƒ
- ä½¿ç”¨BLEUã€chrFç­‰æŒ‡æ ‡è¯„åˆ†

### 6. ğŸ›¡ï¸ å®‰å…¨æ€§ä¸å¯¹é½è¯„ä¼°

#### ToxiGen

**æ¦‚è¿°**ï¼šè¯„ä¼°æ¨¡å‹ç”Ÿæˆæœ‰æ¯’å†…å®¹çš„å€¾å‘

**ç‰¹ç‚¹**ï¼š
- è¦†ç›–å¤šç§æœ‰æ¯’å†…å®¹ç±»åˆ«ï¼ˆä»‡æ¨ã€æ­§è§†ç­‰ï¼‰
- éšå«å’Œæ˜æ˜¾æœ‰æ¯’æç¤ºçš„æ··åˆ
- æµ‹è¯•æ¨¡å‹æ‹’ç»ä¸å½“è¯·æ±‚çš„èƒ½åŠ›

#### Anthropic Harmless

**æ¦‚è¿°**ï¼šè¯„ä¼°æ¨¡å‹é¿å…æœ‰å®³å†…å®¹çš„èƒ½åŠ›

**ç‰¹ç‚¹**ï¼š
- ä»Anthropicçš„RLHFæ•°æ®é›†è¡ç”Ÿ
- æµ‹è¯•å¯¹æœ‰å®³æŒ‡ä»¤çš„æ‹’ç»èƒ½åŠ›
- è¯„ä¼°å›åº”çš„é€‚å½“æ€§å’Œå®‰å…¨æ€§

## ğŸ› ï¸ è¯„ä¼°å·¥å…·ä¸æ¡†æ¶

### 1. ğŸ“Š å¼€æºè¯„ä¼°æ¡†æ¶

**EleutherAI/lm-evaluation-harness**ï¼š
- æ”¯æŒå¤šç§æ¨¡å‹å’ŒåŸºå‡†æµ‹è¯•
- æ ‡å‡†åŒ–è¯„ä¼°æµç¨‹
- å¯æ‰©å±•çš„æ¶æ„è®¾è®¡

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from lm_evaluation.api.registry import get_model, get_task
from lm_evaluation.api.runner import run_task

# åŠ è½½æ¨¡å‹å’Œä»»åŠ¡
model = get_model("hf", model_args={"model": "meta-llama/llama-2-7b"})
task = get_task("mmlu")

# è¿è¡Œè¯„ä¼°
results = run_task(model, task)
print(results)
```

**HELM (Holistic Evaluation of Language Models)**ï¼š
- æ–¯å¦ç¦æ¨å‡ºçš„ç»¼åˆè¯„ä¼°æ¡†æ¶
- å¤šç»´åº¦è¯„ä¼°ï¼ˆå‡†ç¡®æ€§ã€æ¯’æ€§ã€åè§ç­‰ï¼‰
- æ ‡å‡†åŒ–çš„æŠ¥å‘Šç”Ÿæˆ

**OpenAI/evals**ï¼š
- è¯„ä¼°æ¨¡å‹å¯¹æŠ—å„ç§æŒ‘æˆ˜çš„èƒ½åŠ›
- æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•
- æ”¯æŒè‡ªå®šä¹‰è¯„ä¼°

### 2. ğŸ” è¯„ä¼°ç»“æœå¯è§†åŒ–

**é›·è¾¾å›¾**ï¼šå±•ç¤ºæ¨¡å‹åœ¨å¤šä¸ªç»´åº¦çš„æ€§èƒ½
```python
import matplotlib.pyplot as plt
import numpy as np

def radar_plot(models_data, categories):
    # è®¾ç½®é›·è¾¾å›¾
    angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))
    
    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))
    
    for model_name, scores in models_data.items():
        # ç¡®ä¿æ•°æ®é—­åˆ
        values = np.concatenate((scores, [scores[0]]))
        # ç»˜åˆ¶çº¿æ¡
        ax.plot(angles, values, linewidth=2, label=model_name)
        # å¡«å……åŒºåŸŸ
        ax.fill(angles, values, alpha=0.1)
    
    # è®¾ç½®ç±»åˆ«æ ‡ç­¾
    ax.set_thetagrids(angles[:-1] * 180/np.pi, categories)
    
    # æ·»åŠ å›¾ä¾‹
    plt.legend(loc='upper right')
    plt.title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
models_data = {
    "æ¨¡å‹A": [0.85, 0.72, 0.93, 0.65, 0.78],
    "æ¨¡å‹B": [0.92, 0.68, 0.83, 0.75, 0.88]
}
categories = ["MMLU", "GSM8K", "HumanEval", "TruthfulQA", "MT-Bench"]
radar_plot(models_data, categories)
```

**æ’è¡Œæ¦œ**ï¼šæ¨¡å‹é—´çš„æ€§èƒ½æ’åå’Œæ¯”è¾ƒ
- Hugging Face Open LLM Leaderboard
- LMSYS Chatbot Arena
- Papers with CodeåŸºå‡†æµ‹è¯•æ’è¡Œæ¦œ

### 3. ğŸ§ª è¯„ä¼°è®¾è®¡æœ€ä½³å®è·µ

**é¿å…æ•°æ®æ±¡æŸ“**ï¼š
- ä½¿ç”¨æœ€æ–°æˆ–ç§æœ‰æµ‹è¯•é›†
- æ£€æµ‹æµ‹è¯•é›†æ˜¯å¦åœ¨é¢„è®­ç»ƒæ•°æ®ä¸­
- åˆ›å»ºåŠ¨æ€ç”Ÿæˆçš„æµ‹è¯•æ¡ˆä¾‹

**è¯„ä¼°å¤šæ ·æ€§**ï¼š
- åŒ…å«ä¸åŒå›°éš¾ç¨‹åº¦çš„ä»»åŠ¡
- è¦†ç›–å¤šç§èƒ½åŠ›ç»´åº¦
- åŒ…æ‹¬è¾¹ç¼˜å’Œç½•è§æƒ…å†µ

**å‡å°‘åè§**ï¼š
- å¹³è¡¡ä¸åŒæ–‡åŒ–ã€æ€§åˆ«ã€ç§æ—è¡¨ç¤º
- å¤šæ ·åŒ–çš„è¯„ä¼°è€…èƒŒæ™¯
- æ˜ç¡®è¯„åˆ†æ ‡å‡†å’ŒæŒ‡å—

## ğŸ”¬ é«˜çº§è¯„ä¼°æ–¹æ³•

### 1. ğŸ§¬ å¯¹æŠ—æ€§è¯„ä¼°

**æ¦‚å¿µ**ï¼šè®¾è®¡å®¹æ˜“ä½¿æ¨¡å‹å¤±è´¥çš„æµ‹è¯•æ ·æœ¬

**æ–¹æ³•**ï¼š
- è‡ªç„¶è¯­è¨€å¯¹æŠ—æ ·æœ¬ç”Ÿæˆ
- è‡ªåŠ¨åŒ–å¼±ç‚¹æ¢ç´¢
- ç³»ç»Ÿæ€§æ”»å‡»æµ‹è¯•ï¼ˆå¦‚è¶Šç‹±æ”»å‡»ï¼‰

**ç¤ºä¾‹æŠ€æœ¯**ï¼š
```python
def generate_adversarial_prompts(base_prompt, model, n=10):
    """ç”Ÿæˆå¯¹æŠ—æ€§æç¤ºå˜ä½“"""
    adversarial_prompts = []
    
    # è¯·æ±‚æ¨¡å‹ç”Ÿæˆå¯èƒ½å¯¼è‡´åŸæ¨¡å‹å¤±è´¥çš„å˜ä½“
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©æµ‹è¯•AIç³»ç»Ÿå¼±ç‚¹çš„åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": f"ç»™æˆ‘åˆ›å»º{n}ä¸ª'{base_prompt}'çš„å˜ä½“ï¼Œè¿™äº›å˜ä½“è®¾è®¡ä¸ºå¯èƒ½å¯¼è‡´AIæ¨¡å‹ç»™å‡ºé”™è¯¯ã€æœ‰å®³æˆ–ä½è´¨é‡å›ç­”çš„æç¤ºã€‚ä¿æŒåŸå§‹æ„å›¾ä½†ä½¿ç”¨ä¸åŒè¡¨è¾¾æ–¹å¼ã€‚"}
        ]
    )
    
    # è§£æè¿”å›çš„å˜ä½“
    variants = response.choices[0].message.content.strip().split("\n")
    for v in variants:
        if v and len(v) > 10:  # ç®€å•è¿‡æ»¤
            adversarial_prompts.append(v)
    
    return adversarial_prompts
```

### 2. ğŸ“± çœŸå®åœºæ™¯è¯„ä¼°

**æ¦‚å¿µ**ï¼šåœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­è¯„ä¼°æ¨¡å‹è¡¨ç°

**æ–¹æ³•**ï¼š
- ç”¨æˆ·ä½“éªŒç ”ç©¶
- A/Bæµ‹è¯•ä¸åŒæ¨¡å‹
- é•¿æœŸä½¿ç”¨æ•°æ®æ”¶é›†ä¸åˆ†æ

**å…³é”®æŒ‡æ ‡**ï¼š
- ç”¨æˆ·æ»¡æ„åº¦
- ä»»åŠ¡å®Œæˆç‡
- äº¤äº’æŒç»­æ€§
- é”™è¯¯ç‡å’Œæ¢å¤èƒ½åŠ›

### 3. ğŸ§® èƒ½åŠ›æ¢é’ˆè¯„ä¼°

**æ¦‚å¿µ**ï¼šä½¿ç”¨ç‰¹å®šè®¾è®¡çš„ä»»åŠ¡æµ‹è¯•æ¨¡å‹çš„ç‰¹å®šèƒ½åŠ›

**ç¤ºä¾‹æ¢é’ˆä»»åŠ¡**ï¼š
- é€»è¾‘ä¸€è‡´æ€§ï¼šæµ‹è¯•æ¨¡å‹åœ¨ä¸åŒè¡¨è¿°ä¸‹å›ç­”æ˜¯å¦ä¸€è‡´
- å› æœæ¨ç†ï¼šæµ‹è¯•å¯¹å› æœå…³ç³»çš„ç†è§£
- å±æ€§ç»‘å®šï¼šæµ‹è¯•å¯¹å®ä½“åŠå…¶å±æ€§å…³ç³»çš„ç†è§£

**å®ç°æ–¹æ³•**ï¼š
```python
def consistency_probe(model, base_question, paraphrases):
    """è¯„ä¼°æ¨¡å‹åœ¨é—®é¢˜ä¸åŒè¡¨è¿°ä¸‹çš„ä¸€è‡´æ€§"""
    answers = []
    
    # å¯¹åŸå§‹é—®é¢˜å’Œå„ç§æ”¹å†™æé—®
    answers.append(get_model_answer(model, base_question))
    for paraphrase in paraphrases:
        answers.append(get_model_answer(model, paraphrase))
    
    # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
    # ç®€å•å®ç°ï¼šæ£€æŸ¥æ‰€æœ‰ç­”æ¡ˆæ˜¯å¦æŒ‡å‘ç›¸åŒäº‹å®
    consistency_score = measure_semantic_similarity(answers)
    
    return {
        "base_question": base_question,
        "paraphrases": paraphrases,
        "answers": answers,
        "consistency_score": consistency_score
    }
```

## ğŸ“ˆ è¯„ä¼°è¶‹åŠ¿ä¸æœªæ¥æ–¹å‘

### 1. ğŸ”® æ–°å…´è¯„ä¼°æ–¹å‘

**å¤šæ¨¡æ€è¯„ä¼°**ï¼š
- æ–‡æœ¬ä¸å›¾åƒç†è§£ç»“åˆ
- è·¨æ¨¡æ€æ¨ç†èƒ½åŠ›
- ä¸åŒæ¨¡æ€é—´çš„çŸ¥è¯†è¿ç§»

**é•¿æ–‡æœ¬ç†è§£è¯„ä¼°**ï¼š
- é•¿ä¸Šä¸‹æ–‡çª—å£çš„æœ‰æ•ˆåˆ©ç”¨
- æ–‡æ¡£çº§ä¿¡æ¯æ£€ç´¢å’Œæ€»ç»“
- é•¿è·ç¦»ä¾èµ–å…³ç³»ç†è§£

**äº¤äº’å¼è¯„ä¼°**ï¼š
- å¤šè½®å¯¹è¯è´¨é‡è¯„ä¼°
- ä¼šè¯å†å²çš„æœ‰æ•ˆåˆ©ç”¨
- ç”¨æˆ·æ„å›¾ç†è§£ä¸ç»´æŒ

### 2. ğŸ¤– å…ƒè¯„ä¼°é—®é¢˜

**è¯„ä¼°è¯„ä¼°è€…**ï¼š
- è¯„ä¼°æ–¹æ³•çš„å¯é æ€§ç ”ç©¶
- äººç±»è¯„ä»·è€…çš„ä¸€è‡´æ€§åˆ†æ
- æ¨¡å‹è¾…åŠ©è¯„ä¼°çš„åè§ç ”ç©¶

**ç»¼åˆå¾—åˆ†çš„å±€é™æ€§**ï¼š
- å•ä¸€æ•°å­—åˆ†æ•°æ©ç›–å¤šç»´èƒ½åŠ›å·®å¼‚
- ä¸åŒåº”ç”¨åœºæ™¯éœ€è¦ä¸åŒè¯„ä¼°ä¾§é‡
- å¦‚ä½•å¹³è¡¡å„èƒ½åŠ›ç»´åº¦æƒé‡

### 3. ğŸ’¡ æœªæ¥è¯„ä¼°æ¡†æ¶å±•æœ›

**ä¸ªæ€§åŒ–è¯„ä¼°æ¡†æ¶**ï¼š
- åŸºäºåº”ç”¨åœºæ™¯å®šåˆ¶è¯„ä¼°æ–¹æ³•
- é’ˆå¯¹ç‰¹å®šç”¨æˆ·ç¾¤ä½“çš„ç›¸å…³æ€§è¯„ä¼°
- é€‚åº”æ€§è¯„ä¼°æ ‡å‡†

**æŒç»­åŠ¨æ€è¯„ä¼°**ï¼š
- éšæ—¶é—´è¿½è¸ªæ¨¡å‹èƒ½åŠ›æ¼”å˜
- å®šæœŸä¸æ–°åŸºå‡†å’Œæ ‡å‡†å¯¹é½
- é˜²æ­¢è¿‡åº¦ä¼˜åŒ–ç‰¹å®šåŸºå‡†

**ç¤¾åŒºé©±åŠ¨è¯„ä¼°**ï¼š
- å¼€æ”¾åä½œæ„å»ºæ›´å…¨é¢è¯„ä¼°ä½“ç³»
- å¤šå…ƒæ–‡åŒ–å’Œè§‚ç‚¹çº³å…¥è¯„ä¼°
- é™ä½è¯„ä¼°æˆæœ¬å’ŒæŠ€æœ¯é—¨æ§›

## ğŸ“š å»¶ä¼¸èµ„æº

### 1. ğŸ”§ è¯„ä¼°å·¥å…·

- [ğŸ› ï¸ Hugging Face Evaluate](https://github.com/huggingface/evaluate) - æ˜“ç”¨çš„è¯„ä¼°åº“
- [ğŸ› ï¸ LMSYS Chatbot Arena](https://chat.lmsys.org/) - å¯¹è¯æ¨¡å‹ç›²æµ‹å¹³å°
- [ğŸ› ï¸ Stanford HELM](https://github.com/stanford-crfm/helm) - ç»¼åˆè¯„ä¼°æ¡†æ¶
- [ğŸ› ï¸ EleutherAI lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) - å¤§è§„æ¨¡è¯„ä¼°å·¥å…·

### 2. ğŸ“‘ é‡è¦è®ºæ–‡

- [ğŸ“„ "Evaluating Large Language Models Trained on Code"](https://arxiv.org/abs/2107.03374) - HumanEvalè¯„ä¼°
- [ğŸ“„ "Beyond the Imitation Game"](https://arxiv.org/abs/2206.04615) - BIG-Benchè¯„ä¼°
- [ğŸ“„ "Measuring Massive Multitask Language Understanding"](https://arxiv.org/abs/2009.03300) - MMLUåŸºå‡†
- [ğŸ“„ "MT-Bench: A Benchmark for Evaluating LLMs at Multi-turn Conversations"](https://arxiv.org/abs/2305.14498) 