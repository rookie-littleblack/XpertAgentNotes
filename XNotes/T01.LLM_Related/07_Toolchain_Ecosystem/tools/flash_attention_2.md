# Flash Attention 2

- 本质：一种高效的注意力机制实现（高性能CUDA算子），属于“高效注意力/推理加速”技术。

- 作用：极大提升Transformer中注意力计算的速度和显存利用率，支持更长上下文和更大批量。

- 应用阶段：既可用于训练，也可用于推理。

- 归类：
    - 算子优化（Operator Optimization）
    - 高效注意力机制（Efficient Attention）
    - 训练/推理加速（Training/Inference Acceleration）


