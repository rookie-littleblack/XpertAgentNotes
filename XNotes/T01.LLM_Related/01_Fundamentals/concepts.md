# 🔍 大模型核心概念

## 🧠 大型语言模型（LLM）定义

- 🔆 **概念**：大型语言模型是在海量文本数据上训练的神经网络模型，具有理解和生成人类语言的能力
- 📏 **规模特征**：通常拥有数十亿至数千亿参数，训练数据量达TB或PB级别
- 🎯 **核心能力**：通过预测下一个词/token来生成连贯文本，展现出对语言的深度理解

## 🧩 架构与组成

- 🏗️ **Transformer架构**：大多数现代LLM的基础架构，基于自注意力机制
- 🔄 **编码器-解码器结构**：
  - 编码器(Encoder)：擅长理解和表示输入文本
  - 解码器(Decoder)：专注于生成新文本
  - 不同模型可能仅使用其中一部分或两者结合
- 🧮 **参数**：模型中可学习的变量，决定模型的能力上限
- 🧪 **激活函数**：在神经网络层之间引入非线性变换的函数

## 💻 训练与学习方式

- 🔮 **预训练**：在大规模无标注数据上进行自监督学习的初始阶段
- 🎚️ **微调**：在特定任务数据上对预训练模型进行调整
- 📝 **指令微调**：针对遵循指令的能力进行专门优化
- 👥 **RLHF**：通过人类反馈的强化学习，使模型输出更符合人类偏好
- 🌱 **少样本学习**：仅通过少量示例就能完成新任务

## 🎭 模型能力与局限

- 💪 **涌现能力**：随着规模增长突然出现的新能力，如推理、代码生成等
- 🧿 **幻觉**：生成看似合理但实际不正确或不存在的内容
- 🔒 **上下文窗口**：模型能处理的最大输入长度
- 🧭 **对齐问题**：确保模型行为符合人类意图和价值观的挑战

## 📊 评估与衡量

- 📈 **困惑度(Perplexity)**：衡量模型预测下一个token准确性的指标
- ✅ **准确率**：在特定任务上的正确率
- 🎖️ **基准测试**：如MMLU、HumanEval等标准化测试集
- 👁️ **人类评估**：对模型输出质量的主观评价

## 🌐 模型类型分类

- 🔑 **开源vs闭源**：是否公开模型权重和技术细节
- 📦 **通用vs专用**：是否针对特定领域或任务进行优化
- 📏 **规模划分**：
  - 小型模型：通常<10B参数
  - 中型模型：10B-100B参数
  - 大型模型：>100B参数

## 📱 应用范式

- 📝 **提示工程**：设计有效提示以引导模型生成所需输出
- 🔗 **思维链(CoT)**：引导模型逐步推理解决复杂问题
- 🛠️ **工具使用**：让模型调用外部工具和API扩展能力
- 🤖 **智能体框架**：将模型集成到具有记忆、规划和工具使用能力的系统中 